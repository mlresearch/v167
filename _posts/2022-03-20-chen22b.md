---
title: Faster Perturbed Stochastic Gradient Methods for Finding Local Minima
abstract: " Escaping from saddle points and finding local minimum is a central problem
  in nonconvex optimization. Perturbed gradient methods are perhaps the simplest approach
  for this problem. However, to find $(\\epsilon, \\sqrt{\\epsilon})$-approximate
  local minima, the existing best stochastic gradient complexity for this type of
  algorithms is $\\tilde O(\\epsilon^{-3.5})$, which is not optimal. In this paper,
  we propose \\texttt{LENA} (\\textbf{L}ast st\\textbf{E}p shri\\textbf{N}k\\textbf{A}ge),
  a faster perturbed stochastic gradient framework for finding local minima.  We show
  that $\\algname$ with stochastic gradient estimators such as SARAH/SPIDER and STORM
  can find $(\\epsilon, \\epsilon_{H})$-approximate local minima within $\\tilde O(\\epsilon^{-3}
  + \\epsilon_{H}^{-6})$ stochastic gradient evaluations (or $\\tilde O(\\epsilon^{-3})$
  when $\\epsilon_H = \\sqrt{\\epsilon}$). The core idea of our framework is a step-size
  shrinkage scheme to control the average movement of the iterates, which leads to
  faster convergence to the local minima."
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen22b
month: 0
tex_title: Faster Perturbed Stochastic Gradient Methods for Finding Local Minima
firstpage: 176
lastpage: 204
page: 176-204
order: 176
cycles: false
bibtex_author: Chen, Zixiang and Zhou, Dongruo and Gu, Quanquan
author:
- given: Zixiang
  family: Chen
- given: Dongruo
  family: Zhou
- given: Quanquan
  family: Gu
date: 2022-03-20
address:
container-title: Proceedings of The 33rd International Conference on Algorithmic Learning
  Theory
volume: '167'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 3
  - 20
pdf: https://proceedings.mlr.press/v167/chen22b/chen22b.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
