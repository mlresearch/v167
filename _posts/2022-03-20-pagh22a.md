---
title: Infinitely Divisible Noise in the Low Privacy Regime
abstract: Federated learning, in which training data is distributed among users and
  never shared, has emerged as a popular approach to privacy-preserving machine learning.
  Cryptographic techniques such as secure aggregation are used to aggregate contributions,
  like a model update, from all users. A robust technique for making such aggregates
  differentially private is to exploit \emph{infinite divisibility} of the Laplace
  distribution, namely, that a Laplace distribution can be expressed as a sum of i.i.d.Â noise
  shares from a Gamma distribution, one share added by each user. However, Laplace
  noise is known to have suboptimal error in the low privacy regime for $\varepsilon$-differential
  privacy, where $\varepsilon > 1$ is a large constant. In this paper we present the
  first infinitely divisible noise distribution for real-valued data that achieves
  $\varepsilon$-differential privacy and has expected error that decreases exponentially
  with $\varepsilon$.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: pagh22a
month: 0
tex_title: Infinitely Divisible Noise in the Low Privacy Regime
firstpage: 881
lastpage: 909
page: 881-909
order: 881
cycles: false
bibtex_author: Pagh, Rasmus and Stausholm, Nina Mesing
author:
- given: Rasmus
  family: Pagh
- given: Nina Mesing
  family: Stausholm
date: 2022-03-20
address:
container-title: Proceedings of The 33rd International Conference on Algorithmic Learning
  Theory
volume: '167'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 3
  - 20
pdf: https://proceedings.mlr.press/v167/pagh22a/pagh22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
