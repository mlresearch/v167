---
title: Faster Rates of Private Stochastic Convex Optimization
abstract: In this paper, we revisit the problem of Differentially Private Stochastic
  Convex Optimization (DP-SCO) and provide excess population risks for some special
  classes of functions that are faster than the previous results of general convex
  and strongly convex functions. In the first part of the paper, we study the case
  where the population risk function satisfies the Tysbakov Noise Condition (TNC)
  with some parameter $\theta>1$. Specifically, we first show that under some mild
  assumptions on the loss functions, there is an algorithm whose output could achieve
  an upper bound of $\tilde{O}((\frac{1}{\sqrt{n}}+\frac{d}{n\epsilon})^\frac{\theta}{\theta-1})
  $ and $\tilde{O}((\frac{1}{\sqrt{n}}+\frac{\sqrt{d\log(1/\delta)}}{n\epsilon})^\frac{\theta}{\theta-1})$
  for $\epsilon$-DP and $(\epsilon, \delta)$-DP, respectively when $\theta\geq 2$,
  here $n$ is the sample size and $d$ is the dimension of the space. Then we address
  the inefficiency issue, improve the upper bounds by $\text{Poly}(\log n)$ factors
  and extend to the case where $\theta\geq \bar{\theta}>1$ for some known $\bar{\theta}$.
  Next we show that the excess population risk of population functions satisfying
  TNC with parameter $\theta\geq 2$ is always lower bounded by  $\Omega((\frac{d}{n\epsilon})^\frac{\theta}{\theta-1})
  $ and $\Omega((\frac{\sqrt{d\log(1/\delta)}}{n\epsilon})^\frac{\theta}{\theta-1})$
  for $\epsilon$-DP and $(\epsilon, \delta)$-DP, respectively, which matches our upper
  bounds. In the second part, we focus on a special case where the population risk
  function is strongly convex. Unlike the previous studies, here we assume the loss
  function is  non-negative and  the optimal value of population risk is sufficiently
  small. With these additional assumptions, we propose a new method whose output could
  achieve an upper bound of $O(\frac{d\log(1/\delta)}{n^2\epsilon^2}+\frac{1}{n^{\tau}})$
  and $O(\frac{d^2}{n^2\epsilon^2}+\frac{1}{n^{\tau}})$ for any $\tau> 1$ in $(\epsilon,\delta)$-DP
  and $\epsilon$-DP model respectively if the sample size $n$ is sufficiently large.
  These results circumvent their corresponding lower bounds in (Feldman et al., 2020)
  for general strongly convex functions.   Finally, we conduct experiments of our
  new methods on real world data. Experimental results also provide new insights into
  established theories.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: su22a
month: 0
tex_title: Faster Rates of Private Stochastic Convex Optimization
firstpage: 995
lastpage: 1002
page: 995-1002
order: 995
cycles: false
bibtex_author: Su, Jinyan and Hu, Lijie and Wang, Di
author:
- given: Jinyan
  family: Su
- given: Lijie
  family: Hu
- given: Di
  family: Wang
date: 2022-03-20
address:
container-title: Proceedings of The 33rd International Conference on Algorithmic Learning
  Theory
volume: '167'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 3
  - 20
pdf: https://proceedings.mlr.press/v167/su22a/su22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
