---
title: Iterated Vector Fields and Conservatism, with Applications to Federated Learning
abstract: We study whether iterated vector fields (vector fields composed with themselves)
  are conservative. We give explicit examples of vector fields for which this self-composition
  preserves conservatism. Notably, this includes gradient vector fields of loss functions
  associated to some generalized linear models. In the context of federated learning,
  we show that when clients have loss functions whose gradient satisfies this condition,
  federated averaging is equivalent to gradient descent on a surrogate loss function.
  We leverage this to derive novel convergence results for federated learning. By
  contrast, we demonstrate that when the client losses violate this property, federated
  averaging can yield behavior which is fundamentally distinct from centralized optimization.
  Finally, we discuss theoretical and practical questions our analytical framework
  raises for federated learning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: charles22a
month: 0
tex_title: Iterated Vector Fields and Conservatism, with Applications to Federated
  Learning
firstpage: 130
lastpage: 147
page: 130-147
order: 130
cycles: false
bibtex_author: Charles, Zachary and Rush, Keith
author:
- given: Zachary
  family: Charles
- given: Keith
  family: Rush
date: 2022-03-20
address:
container-title: Proceedings of The 33rd International Conference on Algorithmic Learning
  Theory
volume: '167'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 3
  - 20
pdf: https://proceedings.mlr.press/v167/charles22a/charles22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
