---
title: The Mirror Langevin Algorithm Converges with Vanishing Bias
abstract: The technique of modifying the geometry of a problem from Euclidean to Hessian
  metric has proved to be quite effective in optimization, and has been the subject
  of study for sampling. The Mirror Langevin Diffusion (MLD) is a sampling analogue
  of mirror flow in continuous time, and it has nice convergence properties under
  log-Sobolev or Poincare inequalities relative to the Hessian metric. In discrete
  time, a simple discretization of MLD is the Mirror Langevin Algorithm (MLA), which
  was shown to have a biased convergence guarantee with a non-vanishing bias term
  (does not go to zero as step size goes to zero). This raised the question of whether
  we need a better analysis or a better discretization to achieve a vanishing bias.
  Here we study the Mirror Langevin Algorithm and show it indeed has a vanishing bias.
  We apply mean-square analysis to show the mixing time bound for MLA under the modified
  self-concordance condition.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li22b
month: 0
tex_title: The Mirror Langevin Algorithm Converges with Vanishing Bias
firstpage: 718
lastpage: 742
page: 718-742
order: 718
cycles: false
bibtex_author: Li, Ruilin and Tao, Molei and Vempala, Santosh S. and Wibisono, Andre
author:
- given: Ruilin
  family: Li
- given: Molei
  family: Tao
- given: Santosh S.
  family: Vempala
- given: Andre
  family: Wibisono
date: 2022-03-20
address:
container-title: Proceedings of The 33rd International Conference on Algorithmic Learning
  Theory
volume: '167'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 3
  - 20
pdf: https://proceedings.mlr.press/v167/li22b/li22b.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
