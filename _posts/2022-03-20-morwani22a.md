---
title: Inductive Bias of Gradient Descent for Weight Normalized Smooth Homogeneous
  Neural Nets
abstract: We analyze the inductive bias of gradient descent for weight normalized
  smooth homogeneous neural nets, when trained on exponential or cross-entropy loss.
  We analyse both standard weight normalization (SWN) and exponential weight normalization
  (EWN), and show that the gradient flow path with EWN is equivalent to gradient flow
  on standard networks with an adaptive learning rate. We extend these results to
  gradient descent, and establish asymptotic relations between weights and gradients  for
  both SWN and EWN. We also show that EWN causes weights to be updated in a way that
  prefers asymptotic relative sparsity. For EWN, we  provide a finite-time convergence
  rate of the loss with gradient flow and a tight asymptotic convergence rate with
  gradient descent. We demonstrate our results for SWN and EWN on synthetic data sets.
  Experimental results on simple datasets support our claim on sparse EWN solutions,
  even with SGD. This demonstrates its potential applications in learning neural networks
  amenable to pruning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: morwani22a
month: 0
tex_title: Inductive Bias of Gradient Descent for Weight Normalized Smooth Homogeneous
  Neural Nets
firstpage: 827
lastpage: 880
page: 827-880
order: 827
cycles: false
bibtex_author: Morwani, Depen and Ramaswamy, Harish G.
author:
- given: Depen
  family: Morwani
- given: Harish G.
  family: Ramaswamy
date: 2022-03-20
address:
container-title: Proceedings of The 33rd International Conference on Algorithmic Learning
  Theory
volume: '167'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 3
  - 20
pdf: https://proceedings.mlr.press/v167/morwani22a/morwani22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
