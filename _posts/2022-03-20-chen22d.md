---
title: Almost Optimal Algorithms for Two-player Zero-Sum Linear Mixture Markov Games
abstract: We study reinforcement learning for two-player zero-sum Markov games with
  simultaneous moves in the finite-horizon setting, where the transition kernel of
  the underlying Markov games can be parameterized by a linear function over the current
  state, both players’ actions and the next state. In particular, we assume that we
  can control both players and aim to find the Nash Equilibrium by minimizing the
  duality gap. We propose an algorithm Nash-UCRL based on the principle “Optimism-in-Face-of-Uncertainty”.
  Our algorithm only needs to find a Coarse Correlated Equilibrium (CCE), which is
  computationally efficient. Specifically, we show that Nash-UCRL can provably achieve
  an $\tilde{O}(dH\sqrt{T})$ regret, where $d$ is the linear function dimension, $H$
  is the length of the game and $T$ is the total number of steps in the game. To assess
  the optimality of our algorithm, we also prove an $\tilde{\Omega}( dH\sqrt{T})$
  lower bound on the regret. Our upper bound matches the lower bound up to logarithmic
  factors, which suggests the optimality of our algorithm.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen22d
month: 0
tex_title: Almost Optimal Algorithms for Two-player Zero-Sum Linear Mixture Markov
  Games
firstpage: 227
lastpage: 261
page: 227-261
order: 227
cycles: false
bibtex_author: Chen, Zixiang and Zhou, Dongruo and Gu, Quanquan
author:
- given: Zixiang
  family: Chen
- given: Dongruo
  family: Zhou
- given: Quanquan
  family: Gu
date: 2022-03-20
address:
container-title: Proceedings of The 33rd International Conference on Algorithmic Learning
  Theory
volume: '167'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 3
  - 20
pdf: https://proceedings.mlr.press/v167/chen22d/chen22d.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
