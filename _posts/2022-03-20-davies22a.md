---
title: Lower Bounds on the Total Variation Distance Between Mixtures of Two Gaussians
abstract: Mixtures of high dimensional Gaussian distributions have been studied extensively
  in statistics and learning theory.  While the total variation distance appears naturally
  in the sample complexity of distribution learning, it is analytically difficult
  to obtain tight lower bounds for mixtures. Exploiting  a connection between total
  variation  distance and the  characteristic function of the mixture, we provide
  fairly tight functional approximations. This enables us to derive new lower bounds
  on the total variation distance between two-component Gaussian mixtures with a shared
  covariance matrix.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: davies22a
month: 0
tex_title: Lower Bounds on the Total Variation Distance Between Mixtures of Two Gaussians
firstpage: 319
lastpage: 341
page: 319-341
order: 319
cycles: false
bibtex_author: Davies, Sami and Mazumdar, Arya and Pal, Soumyabrata and Rashtchian,
  Cyrus
author:
- given: Sami
  family: Davies
- given: Arya
  family: Mazumdar
- given: Soumyabrata
  family: Pal
- given: Cyrus
  family: Rashtchian
date: 2022-03-20
address:
container-title: Proceedings of The 33rd International Conference on Algorithmic Learning
  Theory
volume: '167'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 3
  - 20
pdf: https://proceedings.mlr.press/v167/davies22a/davies22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
